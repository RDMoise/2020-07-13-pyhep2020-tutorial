{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to PyHEP 2020!\n",
    "\n",
    "This is the third workshop in the series, and it's already a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar([\"2018\", \"2019\", \"2020\"], [68, 55, 1000])\n",
    "plt.ylabel(\"Number of participants\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv(\"~/tmp/pyhep2020-survey-results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_collaboration_names = lambda x: {\n",
    "    \"alice\": \"ALICE\",\n",
    "    \"Alice\": \"ALICE\",\n",
    "    \"ALICE\": \"ALICE\",\n",
    "    \"ALICE member\": \"ALICE\",\n",
    "    \"ATLAS\": \"ATLAS\",\n",
    "    \"ATLAS, BELLE2\": \"ATLAS;BELLE\",\n",
    "    \"ATLAS, Darkside\": \"ATLAS;DarkSide\",\n",
    "    \"ATLAS, FCC, IDEA\": \"ATLAS;FCC;IDEA\",\n",
    "    \"ATLAS, IRIS-HEP\": \"ATLAS;IRIS-HEP\",\n",
    "    \"ATLAS, KM3NeT\": \"ATLAS;KM3NeT\",\n",
    "    \"BaBar, HFLAV, PDG, LHCb, FNAL-E989\": \"BaBar;HFLAV;PDG;LHCb;g-2\",\n",
    "    \"Belle\": \"BELLE\",\n",
    "    \"BELLE\": \"BELLE\",\n",
    "    \"Belle2\": \"BELLE\",\n",
    "    \"Belle, ARA\": \"BELLE;ARA\",\n",
    "    \"Belle / Belle II\": \"BELLE\",\n",
    "    \"Belle, Belle II\": \"BELLE\",\n",
    "    \"belle II\": \"BELLE\",\n",
    "    \"Belle II\": \"BELLE\",\n",
    "    \"Belle II and GAMBIT\": \"BELLE;GAMBIT\",\n",
    "    \"Belle II, CMS\": \"BELLE;CMS\",\n",
    "    \"BELLE I & II\": \"BELLE\",\n",
    "    \"BNL-STAR collaboration\": \"STAR\",\n",
    "    \"cms\": \"CMS\",\n",
    "    \"Cms\": \"CMS\",\n",
    "    \"CMS\": \"CMS\",\n",
    "    \"CMS, ALICE\": \"CMS;ALICE\",\n",
    "    \"CMS, DUNE\": \"CMS;DUNE\",\n",
    "    \"CMS Experiment\": \"CMS\",\n",
    "    \"CMS, SModelS\": \"CMS;SModelS\",\n",
    "    \"COMET\": \"COMET\",\n",
    "    \"COMET MEG\": \"COMET;MEG\",\n",
    "    \"COMPASS\": \"COMPASS\",\n",
    "    \"CRESST, COSINUS\": \"CRESST;COSINUS\",\n",
    "    \"CUORE/CUPID\": \"CUORE\",\n",
    "    \"DarkSide\": \"DarkSide\",\n",
    "    \"DUNE\": \"DUNE\",\n",
    "    \"DUNE (SAND), NICA\": \"DUNE;NICA\",\n",
    "    \"I am currently associated with Belle and Belle 2 experiments.\": \"BELLE\",\n",
    "    \"I am in a team that they are in collaboration with CMS and I am going to join in a CERN group.\": \"CMS\",\n",
    "    \"ICARUS and DUNE\": \"ICARUS;DUNE\",\n",
    "    \"I'm associated with CMS experiment\": \"CMS\",\n",
    "    \"ISOLDE\": \"ISOLDE\",\n",
    "    \"Iuac\": \"Iuac\",\n",
    "    \"Just switched from CMS to CTA\": \"CTA\",\n",
    "    \"KEK\": \"KEK\",\n",
    "    \"key4hep\": \"key4hep\",\n",
    "    \"lhcb\": \"LHCb\",\n",
    "    \"LHCb\": \"LHCb\",\n",
    "    \"LHCb SHiP\": \"LHCb;SHiP\",\n",
    "    \"LHCb, SHiP\": \"LHCb;SHiP\",\n",
    "    \"LHCb, TORCH\": \"LHCb;TORCH\",\n",
    "    \"LUX, NEST\": \"LUX;NEST\",\n",
    "    \"Mu2e\": \"Mu2e\",\n",
    "    \"Mu2e, a few small instrumentation efforts outside Mu2e\": \"Mu2e\",\n",
    "    \"Mu2e, Muon g-2\": \"Mu2e;g-2\",\n",
    "    \"Mu2e, NEWS-G\": \"Mu2e;NEWS-G\",\n",
    "    \"MUon RAdiography of Mt VESuvius (MURAVES)\": \"MURAVES\",\n",
    "    \"NA61/SHINE Experiment\": \"NA61\",\n",
    "    \"NOvA\": \"NOvA\",\n",
    "    \"NOvA, DUNE\": \"NOvA;DUNE\",\n",
    "    \"RENO,BELLE2, etc\": \"RENO;BELLE\",\n",
    "    \"ROOT\": \"ROOT\",\n",
    "    \"STAR\": \"STAR\",\n",
    "    \"SuperCDMS\": \"SuperCDMS\",\n",
    "    \"Super CDMS\": \"SuperCDMS\",\n",
    "    \"Super-Kamiokande, T2K, DUNE\": \"Super-Kamiokande;T2K;DUNE\",\n",
    "    \"T2K\": \"T2K\",\n",
    "    \"UCNA\": \"UCNA\",\n",
    "    \"XENON (direct dark matter search)\": \"XENON\",\n",
    "    \"XENON    NEST\": \"XENON;NEST\",\n",
    "    \"Yes. Belle and Belle II\": \"BELLE\",\n",
    "    \"Yes, Belle II collaboration. :)\": \"BELLE\",\n",
    "    \"yes, CMS\": \"CMS\",\n",
    "    \"Yes: CMS\": \"CMS\",\n",
    "    \"Yes, with ATLAS.\": \"ATLAS\",\n",
    "}.get(x, \"Other or none\")\n",
    "pandas.DataFrame(df[\n",
    "    [\"Professional life: Are you associated with one or more experimental or theoretical collaborations? (E.g. ATLAS, CMS, DUNE, USQCD...)\"]\n",
    "].applymap(fix_collaboration_names).iloc[:, 0].str.split(\";\").tolist()).stack().value_counts(ascending=True).plot.barh(figsize=(10, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"collaboration-logos.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\n",
    "    \"General physics (student)\",\n",
    "    \"High-energy collider physics\",\n",
    "    \"Neutrino physics\",\n",
    "    \"Physics of nuclei or exotic atoms\",\n",
    "    \"Precision frontier\",\n",
    "    \"Direct dark matter searches\",\n",
    "    \"Astroparticle physics\",\n",
    "    \"Astronomy\",\n",
    "    \"Theory/simulations\",\n",
    "    \"Instrumentation\",\n",
    "    \"Other, not listed above\",\n",
    "]\n",
    "def explode(responses):\n",
    "    responses = [response.strip() for response in responses.split(\";\")]\n",
    "    return [1.0 if option in responses else 0.0 for option in options]\n",
    "exploded = df[[\"Professional life: If you're involved in physics, what area(s) do you study?\"]].fillna(\"\").applymap(explode)\n",
    "indicator = pandas.DataFrame(exploded.iloc[:, 0].tolist(), columns=options)\n",
    "indicator.div(indicator.sum(axis=1), axis=0).sum(axis=0).iloc[::-1].plot.barh();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_country_names(name):\n",
    "    name = \" \".join(x.capitalize() for x in str(name).strip(\" '\\\"\").split())\n",
    "    name = {\n",
    "        \"Brasil\": \"Brazil\",\n",
    "        \"Canada (east)\": \"Canada\",\n",
    "        \"Canada (montreal)\": \"Canada\",\n",
    "        \"Ch\": \"Switzerland\",\n",
    "        \"Czech Rep.\": \"Czech Republic\",\n",
    "        \"Czechia\": \"Czech Republic\",\n",
    "        \"France (cern-based)\": \"France\",\n",
    "        \"Greeve\": \"Greece\",\n",
    "        \"MÃ©xico\": \"Mexico\",\n",
    "        \"Netherlands. Time Slot Also Dependent On Another Conference. So Need To Be Able To Attend Both.\": \"Netherlands\",\n",
    "        \"Osaka, Japan\": \"Japan\",\n",
    "        \"Republic Of Korea\": \"South Korea\",\n",
    "        \"Russia Federation\": \"Russia\",\n",
    "        \"S.korea\": \"South Korea\",\n",
    "        \"Stockholm\": \"Sweden\",\n",
    "        \"Taiwan (r.o.c.)\": \"Taiwan\",\n",
    "        \"The Netherlands\": \"Netherlands\",\n",
    "        \"The U.s\": \"United States\",\n",
    "        \"Uk\": \"United Kingdom\",\n",
    "        \"U.s.\": \"United States\",\n",
    "        \"U.s.a.\": \"United States\",\n",
    "        \"United State\": \"United States\",\n",
    "        \"Us\": \"United States\",\n",
    "        \"Us (est)\": \"United States\",\n",
    "        \"Usa\": \"United States\",\n",
    "        \"Usa (chicago)\": \"United States\",\n",
    "        \"Usa - Michigan\": \"United States\",\n",
    "        \"United States Of America\": \"United States\",\n",
    "        \"United Status\": \"United States\",\n",
    "        \"Nan\": \"Prefer not to say\",\n",
    "    }.get(name, name)\n",
    "    return name\n",
    "df[[\"PyHEP feedback: In what country do you currently reside?\"]].applymap(fix_country_names).iloc[:, 0].value_counts(ascending=True).plot.barh(figsize=(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/using-python-to-create-a-world-map-from-a-list-of-country-names-cd7480d03b10\n",
    "import folium\n",
    "import folium.plugins\n",
    "import pycountry_convert\n",
    "import geopy\n",
    "\n",
    "geolocator = geopy.geocoders.Nominatim(user_agent=\"PyHEP2020\")\n",
    "world_map = folium.Map(tiles=\"cartodbpositron\")\n",
    "marker_cluster = folium.plugins.MarkerCluster().add_to(world_map)\n",
    "countries = df[[\"PyHEP feedback: In what country do you currently reside?\"]].applymap(fix_country_names).iloc[:, 0].value_counts()\n",
    "for country, count in countries.items():\n",
    "    try:\n",
    "        two_letter = pycountry_convert.country_name_to_country_alpha2(country)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        loc = geolocator.geocode(two_letter)\n",
    "        for i in range(count):\n",
    "            folium.CircleMarker(location=(loc.latitude, loc.longitude), radius=5, fill=True).add_to(marker_cluster)\n",
    "        print(f\"{count} in {country} (lat {loc.latitude} lng {loc.longitude})\")\n",
    "\n",
    "world_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\n",
    "    \"PyHEP feedback: Atlantic: 15:00 CET, 06:00 PDT, 18:30 IST, 22:00 JST\",\n",
    "    \"PyHEP feedback: Indian Ocean: 09:00 CET, 00:00 PDT, 12:30 IST, 16:00 JST\",\n",
    "    \"PyHEP feedback: Pacific: 00:00 CET, 15:00 PDT, 03:30 IST, 07:00 JST\",\n",
    "]].apply(pandas.Series.value_counts).loc[[\"Great!\", \"Acceptable\", \"BAD\"]].plot.bar(rot=0).legend(bbox_to_anchor=(1.2, 0.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopes = [\n",
    "    \"Particle physics analysis tools (other than ROOT)\",\n",
    "    \"General-purpose data analysis toolkits\",\n",
    "    \"Machine learning/deep learning toolkits\",\n",
    "    \"Software engineering skills (beyond the fundamentals)\",\n",
    "    \"ROOT and PyROOT\",\n",
    "    \"Python fundamentals (how to program in Python)\",\n",
    "    \"Collaboration-specific topics\",\n",
    "    \"Other\",\n",
    "]\n",
    "def explode(responses):\n",
    "    responses = [response.strip() for response in responses.split(\";\")]\n",
    "    return [1.0 if hope in responses else 0.0 for hope in hopes]\n",
    "exploded = df[[\"PyHEP feedback: What are you hoping to learn from this workshop?\"]].fillna(\"\").applymap(explode)\n",
    "indicator = pandas.DataFrame(exploded.iloc[:, 0].tolist(), columns=hopes)\n",
    "indicator.div(indicator.sum(axis=1), axis=0).sum(axis=0).iloc[::-1].plot.barh();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're in the right place!\n",
    "\n",
    "**Keynotes:**\n",
    "\n",
    "   * Rubin Observatory: the software behind the science _(Nate Lust)_\n",
    "   * Python & HEP: a perfect match, in theory _(David Straub)_\n",
    "\n",
    "**Tutorials:**\n",
    "\n",
    "   * Uproot & Awkward Arrays _(Jim Pivarski)_\n",
    "   * Jagged physics analysis with Numba, Awkward, and Uproot on a GPU _(Joosep Pata)_\n",
    "   * Ganga: flexible virtualization for user-based large computations _(Ulrik Egede)_\n",
    "   * A prototype U.S. CMS analysis facility _(Oksana Shadura)_\n",
    "   * Columnar analysis at scale with Coffea _(Mat Adamec)_\n",
    "   * Introduction to automatic differentiation _(Lukas Heinrich)_\n",
    "   * High-performance Python _(Henry Schreiner)_\n",
    "   * Model-building & statistical inference with zfit and hepstats _(Jonas Eschle)_\n",
    "   * pyhf: accelerating analyses and preserving likelihoods _(Matt Feickert)_\n",
    "   * ThickBrick: optimal event selection and categorization in HEP _(Prasanth Shyamsundar)_\n",
    "\n",
    "**Talks:**\n",
    "\n",
    "   * NanoEvents object _(Nick Smith)_\n",
    "   * TITANIA: how to structure dector monitoring _(Jakub Kowalski, Maciej Witold Majewski)_\n",
    "   * A new PyROOT for ROOT 6.22 _(Enric Tejedor Saavedra)_\n",
    "   * Resample: bootstrap and jackknife from Python _(Hans Dembinski)_\n",
    "   * Design pattern for analysis automation using Luigi _(Marcel Rieger)_\n",
    "   * ServiceX: on-demand data transformation & delivery _(Kyungeon Choi)_\n",
    "   * Integrating Coffea and WorkQueue _(Cami Carballo)_\n",
    "   * High granularity calorimeter (HGCAL) test beam analysis using Jupyter _(Matteo Bonanomi)_\n",
    "   * neos: physics analysis as a differentiable program _(Nate Simpson)_\n",
    "   * SModelS: a tool for interpreting simplified-model results _(Wolfgang Waltenberger)_\n",
    "   * TensorFlow-based maximum likelihood fits for high-precision Standard Model measurements at CMS _(Josh Bendavid)_\n",
    "   * Error computation in iminuit and MINUIT: how HESSE and MINOS work _(Hans Dembinski)_\n",
    "   * zfit with TensorFlow 2.0: dynamic and compiled HPC _(Jonas Eschle)_\n",
    "   * Machine learning for signal-background separation of nuclear interaction vertices in CMS _(Anna Kropivnitskaya)_\n",
    "   * The boost-histogram package _(Henry Schreiner)_\n",
    "   * Providing Python bindings for complex and feature-rich C and C++ libraries _(Martin Schwinzerl)_\n",
    "   * Integrating GPU libraries for fun and profit _(Adrian Oeftiger)_\n",
    "   * mplhep: bridging Matplotlib and HEP _(Andrzej Novak)_\n",
    "   * ROOT preprocessing pipeline for machine learning with TensorFlow _(Matthias Komm)_\n",
    "   * Integrated data acquisition in Python _(Charles Burton)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
